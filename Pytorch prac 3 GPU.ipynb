{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n",
      "24946\n",
      "2494\n",
      "22452\n",
      "2494\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))\n",
    "\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]\n",
    "\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(\"Accuracy:\", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:11<00:00, 19.80it/s]\n",
      "  1%|▏         | 3/225 [00:00<00:08, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1935, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:09<00:00, 23.53it/s]\n",
      "  1%|▏         | 3/225 [00:00<00:08, 25.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1638, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:09<00:00, 24.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1469, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "In-sample acc: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = net.to(device)\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "\n",
    "            matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, batch_y)]\n",
    "            in_sample_acc = matches.count(True)/len(matches)\n",
    "\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "        print(\"In-sample acc:\",round(in_sample_acc, 2))\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "def batch_test(net):\n",
    "    BATCH_SIZE = 100\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        #np.random.shuffle(test_X)\n",
    "        #np.random.shuffle(test_y)\n",
    "\n",
    "        batch_X = test_X[:BATCH_SIZE].view(-1,1,50,50)\n",
    "        batch_y = test_y[:BATCH_SIZE]\n",
    "\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "\n",
    "        matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, batch_y)]\n",
    "        acc = matches.count(True)/len(matches)\n",
    "\n",
    "        print(\"Test Accuracy:\", round(acc, 3))\n",
    "\n",
    "\n",
    "batch_test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(X, y, train=False):\n",
    "\n",
    "    if train:\n",
    "        net.zero_grad()\n",
    "    outputs = net(X)\n",
    "    matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.46  Loss: 0.2503\n",
      "Acc: 0.54  Loss: 0.2491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.54  Loss: 0.2482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "            print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "\n",
    "            # just to show the above working, and then get out:\n",
    "            if i == 5:\n",
    "                break\n",
    "            break\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "\n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                f.write(f\"{MODEL_NAME},{int(time.time())},in_sample,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i == 5:\n",
    "                    break\n",
    "                break\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                f.write(f\"{MODEL_NAME},{round(time.time(),3)},in_sample,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i == 5:\n",
    "                    break\n",
    "                break\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAeCklEQVR4nO3de0xU19oG8GdmNmIRRJhRKamalmBqbaqV8VJNPFBIa9RDjYnmi72dQi9KbeVUayte0GNUqhGtgsVYQrVpUw9pm1g9KgFqsFKNiNRSGwXhVAhjCBeVDmfkstf3R+PUEXAuzHX1+f2jm72Y/b4OPO5Ze88ajRBCgIiIAp7W1wUQEZF7MNCJiCTBQCcikgQDnYhIEgx0IiJJMNCJiCSh+PLgTU1NXj2ewWBAS0uLV4/pLTL3BsjdH3sLXL7oLzo6esB9dgN93759qKysRHh4OHbu3NlnvxACBQUFuHjxIoKDg5GWlobHHntscBUTEZHT7E65xMfHIyMjY8D9Fy9exI0bN7Bnzx68+eab+PTTT91aIBEROcZuoD/xxBMIDQ0dcH9FRQVmz54NjUaD8ePHw2w2o7293a1FEhGRfYOeQ29ra4PBYLBu6/V6tLW1ISIios/Y4uJiFBcXAwCysrJsvg/4Y/qmra0NPT09gy2rX83NzZB1pQN396YoCiIjI6HRaNz2mIOhKEqfnxdZsLfA5W/9DTrQ+wuRgUIgKSkJSUlJ1u37Lyb873//Q1BQEBTFM9dqFUXx2H8Wvubu3rq7u9HY2IiHHnrIbY85GDJfXGNvgcvfLooO+rZFvV5v01Bra2u/Z+eOUFXVY2FOzlEUBaqq+roMInLCoAPdaDSirKwMQghcvXoVISEhLge6v7y8pz/w+SAKLHZPh3fv3o3Lly+jo6MDS5cuxeLFi60v7Z977jk8/fTTqKysxLvvvoshQ4YgLS3N40UTEVFfdgM9PT39gfs1Gg1ef/11txVERESu4Vv/75OcnOzrEoiIXMJAv8+RI0d8XQIRkUv89pYS9asDEA317n3QcTHA4tQHDomNjUVNTQ3Ky8uRnZ2NiIgIXLlyBU899RT27t074IXCrVu3oqioCIqiYPbs2diwYQOKioqwZ88edHV1ISIiAjk5ORg5ciR27tyJ69evo7m5GXV1dcjMzERlZSW+//57REVF4bPPPkNQUBCmT5+O5ORklJeXAwBycnLw6KOP9nv8kydPYteuXX2OZTabsW7dOly6dAkajQb//Oc/MW/ePHz//ffIyspCb28vIiMj8e9//3tw/7ZE5HN+G+j+oLq6GqWlpYiKisILL7yA8+fPY9q0aX3Gtbe34/jx4ygrK4NGo8GtW7cAANOmTcN3330HjUaDL7/8Evv27UNmZiYA4LfffkNhYSGuXr2K5ORkHDhwAOvWrUNqaipKSkowZ84cAEBoaCiOHTuGwsJCZGZm4tChQ/3WOn369H6PtXv3boSFhaGkpAQAcPPmTbS2tuL999/HN998g7Fjx/KdvUSS8NtA1/7fG25/TGfffDN58mTrTfwTJ05EQ0NDv4EeFhaG4OBgrFq1ComJidY3T5lMJixbtgzNzc3o6urC2LFjrd+TkJCAoKAgTJgwAaqqIiEhAQDw+OOPo6GhwTpuwYIF1j83btw4YK1NTU3IzMzsc6zTp09j37591nEjRoxAUVERZsyYYR3j6m2mRORfOIf+AEOGDLH+XafTDfifgaIoOHbsGObOnYsTJ07gxRdfBACsX78er732GkpKSvDRRx/hzp071u8JDg4GAGi1WiiKYp3K0Wq16O3ttY67d4rnQfeFr127tt9jCSH6/T7eY04kHwa6G5jNZnR0dCAxMRGbNm3C5cuXAQC3b99GVFQUAKCwsNClx757kfbIkSOIi4sbcNxAx/rb3/6GgoIC6/bNmzcRFxeHH3/8EdevXwcATrkQScJvp1wCye+//46UlBTcuXMHQgjrPPnKlSvx1ltvISoqClOmTLGZSnFUV1cX5s+fD1VVkZubO+C4VatW9XusFStWICMjA88++yy0Wi3ee+89zJ07F9u3b8frr78OVVVhMBjw1VdfudY8EfkNjfDh8oP3f2JRZ2cnQkJCPHa8QFuca/r06Th+/DgiIyPtjvVEb55+Ppwh8yJP7C1wSbc4FxER+QdOuTgpNTXVOvd819q1axEfH+/2Y507d67P1z7++GMcPXrU5mvz58/HypUr3X58IgosfjXlYjabMWzYMI8dL9CmXJzhid48/Xw4Q+aX7uwtcHHK5QG0Wq20gRtoenp6oNX61Y8HEdnhV1MuQ4cOhcViwZ07dzxyn3RwcLDNveAycWdvQghotVoMHTrULY9HRN7hV4Gu0Wg8+pFnMr/8k7k3InIMX1MTEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUlCcWRQVVUVCgoKoKoqEhMTsWDBApv9p06dwueff47IyEgAwJw5c5CYmOj+aomIaEB2A11VVeTn52PdunXQ6/VYs2YNjEYjHnnkEZtxM2fORGpqqscKJSKiB7M75VJbW4uoqCiMHj0aiqJg5syZOH/+vDdqIyIiJ9g9Q29ra4Ner7du6/V61NTU9Bl37tw5/Prrr3j44Yfx6quvwmAw9BlTXFyM4uJiAEBWVla/YzxJURSvH9NbZO4NkLs/9ha4/K0/u4EuhOjzNY1GY7MdFxeHWbNmISgoCEVFRcjNzUVmZmaf70tKSkJSUpJ1u6WlxZWaXWYwGLx+TG+RuTdA7v7YW+DyRX/R0dED7rM75aLX69Ha2mrdbm1tRUREhM2YsLAwBAUFAfgjtOvq6lytlYiIXGQ30GNiYmAymdDc3Iyenh6Ul5fDaDTajGlvb7f+vaKios8FUyIi8jy7Uy46nQ4pKSnYsmULVFVFQkICxowZg8OHDyMmJgZGoxHHjx9HRUUFdDodQkNDkZaW5o3aiYjoHhrR3yS5lzQ1NXn1eDLP58ncGyB3f+wtcAXcHDoREQUGBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJgoFORCQJBjoRkSQY6EREkmCgExFJQnFkUFVVFQoKCqCqKhITE7FgwQKb/d3d3cjJyUFdXR3CwsKQnp6OUaNGeaRgIiLqn90zdFVVkZ+fj4yMDOzatQtnzpxBY2OjzZjS0lIMGzYMe/fuxbx58/DFF194rGAiIuqf3UCvra1FVFQURo8eDUVRMHPmTJw/f95mTEVFBeLj4wEAM2bMQHV1NYQQHimYiIj6Z3fKpa2tDXq93rqt1+tRU1Mz4BidToeQkBB0dHRg+PDhNuOKi4tRXFwMAMjKyoLBYBh0A85QFMXrx/QWmXsD5O6PvQUuf+vPbqD3d6at0WicHgMASUlJSEpKsm63tLQ4VKS7GAwGrx/TW2TuDZC7P/YWuHzRX3R09ID77E656PV6tLa2WrdbW1sREREx4Jje3l50dnYiNDTU1XqJiMgFdgM9JiYGJpMJzc3N6OnpQXl5OYxGo82YuLg4nDp1CgBw9uxZTJw4sd8zdCIi8hyNcODqZWVlJQ4ePAhVVZGQkICFCxfi8OHDiImJgdFoRFdXF3JyclBfX4/Q0FCkp6dj9OjR3qifiIjuEn8hH3zwga9L8BiZexNC7v7YW+Dyt/74TlEiIkkw0ImIJKHbuHHjRl8X4U2PPfaYr0vwGJl7A+Tuj70FLn/qz6GLokRE5P845UJEJAkGOhGRJBxaPjfQyLzcr73ejh49ipKSEuh0OgwfPhzLli3DyJEjfVSt8+z1d9fZs2eRnZ2Nbdu2ISYmxstVusaR3srLy1FYWAiNRoNx48ZhxYoVPqjUefZ6a2lpQW5uLsxmM1RVxZIlSzBlyhQfVeucffv2obKyEuHh4di5c2ef/UIIFBQU4OLFiwgODkZaWprv5tV9fNuk2/X29orly5eLGzduiO7ubrFq1SrR0NBgM+bEiRNi//79QgghfvjhB5Gdne2LUp3mSG8///yzsFgsQgghTp48GTC9CeFYf0II0dnZKTZs2CAyMjJEbW2tDyp1niO9NTU1iffff190dHQIIYS4efOmL0p1miO95eXliZMnTwohhGhoaBBpaWm+KNUlv/zyi7h27Zp47733+t1/4cIFsWXLFqGqqrhy5YpYs2aNlyv8k3RTLjIv9+tIb08++SSCg4MBALGxsWhra/NFqS5xpD8AOHz4MJKTkxEUFOSDKl3jSG8lJSV4/vnnresghYeH+6JUpznSm0ajQWdnJwCgs7Ozz3pQ/uyJJ5544NpUFRUVmD17NjQaDcaPHw+z2Yz29nYvVvgn6QK9v+V+7w+1gZb79XeO9Hav0tJSTJ482RuluYUj/dXX16OlpQVxcXHeLm9QHOmtqakJJpMJ69evx9q1a1FVVeXtMl3iSG+LFi3C6dOnsXTpUmzbtg0pKSneLtNj2trabJbQtfd76UnSBXp/Z9quLvfrb5ypu6ysDHV1dUhOTvZ0WW5jrz9VVXHw4EG88sor3izLLRx57lRVhclkQmZmJlasWIG8vDyYzWZvlegyR3o7c+YM4uPjkZeXhzVr1mDv3r1QVdVbJXqUP+WJdIEu83K/jvQGAJcuXcK3336L1atXB9S0hL3+LBYLGhoasGnTJrz99tuoqanB9u3bce3aNV+U6xRHnrvIyEhMnToViqJg1KhRiI6Ohslk8napTnOkt9LSUjzzzDMAgPHjx6O7uzsgXhU7Qq/X26yJPtDvpTdIF+gyL/frSG/19fU4cOAAVq9eHTBzsHfZ6y8kJAT5+fnIzc1Fbm4uYmNjsXr16oC4y8WR527atGmorq4GANy+fRsmkykgVi11pDeDwWDtrbGxEd3d3X0+0SxQGY1GlJWVQQiBq1evIiQkxGeBLuU7RWVe7tdeb5s3b8b169cxYsQIAH/8In3wwQc+rtpx9vq718aNG/Hyyy8HRKAD9nsTQuDQoUOoqqqCVqvFwoULMWvWLF+X7RB7vTU2NmL//v2wWCwAgJdeegmTJk3ycdWO2b17Ny5fvoyOjg6Eh4dj8eLF6OnpAQA899xzEEIgPz8fP/30E4YMGYK0tDSf/UxKGehERH9F0k25EBH9VTHQiYgkwUAnIpIEA52ISBJ2F+fy5MI0TU1Nzlc8CAaDweZ+UZnI3Bsgd3/sLXD5or/o6OgB99k9Q4+Pj0dGRsaA+y9evIgbN25gz549ePPNN/Hpp5+6ViUREQ2K3UAPpIVpiIj+yga9HvpAC9P0906p4uJiFBcXAwCysrJsvs8bFEXx+jG9RebeALn7Y2+By9/6G3SgO7MwTVJSEpKSkqzb9889CSFgsVigqqpH3oofHByMO3fuuP1x/YEnehNCQKvVYujQoT5fGkHmuVj2Frj8bQ590IHuzoVpLBYLgoKCoCie+SAlRVGg0+k88ti+5qneenp6YLFY8NBDD7n9sYnIvQZ926I7F6ZRVdVjYU6uURRFmmVOiWRnNz3vXZhm6dKlfRamefrpp1FZWYl3333XujCNq3z9sp76x+eFKDDYDfT09PQH7tdoNHj99dfdVhAREbmG7xQlIpIEA/0+gfSRbffasWMH8vLyfF0GEfmQ316BVL86ANFQ794HHRcDLE594JAjR46495hERF7it4HuK7GxsaipqUF5eTmys7MRERGBK1eu4KmnnsLevXsHvEC4detWFBUVQVEUzJ49Gxs2bEBRURH27NmDrq4uREREICcnByNHjsTOnTtx/fp1NDc3o66uDpmZmaisrMT333+PqKgofPbZZwgKCsL06dORnJyM8vJyAEBOTg4effRRuz1UV1fjww8/hMViwbhx47Bz506MGDEC+fn5+Pzzz6EoCmJjY/HJJ5/gxx9/xIYNGwD8cT3km2++CYjPVyWivvw20LX/94bbH1NRFOsdOo6orq5GaWkpoqKi8MILL+D8+fOYNm1an3Ht7e04fvw4ysrKoNFocOvWLQB/fEbkd999B41Ggy+//BL79u1DZmYmAOC3335DYWEhrl69iuTkZBw4cADr1q1DamoqSkpKMGfOHABAaGgojh07hsLCQmRmZuLQoUN2605PT8fmzZvxzDPPYMeOHcjOzsa//vUv5Obm4scff0RwcLC1xry8PGzduhVTp06F2WxGcHCww/8+RORfOIf+AJMnT0Z0dDS0Wi0mTpyIhoaGfseFhYUhODgYq1atwn/+8x/rm3BMJhOWLFmCxMREfPLJJ7h69ar1exISEhAUFIQJEyZYP4cRAB5//HGb4yxYsMD654ULF+zWfPv2bdy6dcv6CeuLFi3CuXPnAAATJkzA8uXL8fXXX1vv9586dSo2bdqE/Px83Lp1i+8DIApgDPQHGDJkiPXvOp1uwLN7RVFw7NgxzJ07FydOnMCLL74IAFi/fj1ee+01lJSU4KOPPrJ5a/7dM2GtVgtFUaxTOVqtFr29vdZx907xDPZ+8EOHDuEf//gHLl26hDlz5qCnpwfLly/Hjh07YLFY8Pe//x21tbWDOgYR+Q4D3Q3MZjM6OjqQmJiITZs24fLlywD+OFuOiooCABQWFrr02Hcv0h45cgRxcXF2xw8fPhzh4eHWs/Kvv/4aM2bMgKqqaGpqwqxZs7Bu3Trcvn0bZrMZ//3vfzFhwgS8/fbbmDRpEgOdKIDx9bUb/P7770hJScGdO3cghLDOk69cuRJvvfUWoqKiMGXKlAGnbB6kq6sL8+fPh6qqyM3Ndeh7du/ebb0oOnbsWGRnZ6O3txfvvPMOOjo6IITAG2+8gfDwcOzYsQPl5eXQarUYP368deqHiAKPRvS3XKKX3P+JRZ2dnQgJCfHY8Zy9KOpr06dPx/HjxxEZGWl3rCd78/Tz4giZV+1jb4HL31Zb5JQLEZEkOOXipNTUVFy/ft3ma2vXrkV8fLzbj3V3HvxeH3/8MY4ePWrztfnz52PlypVuPz4RBRa/mnIxm80YNmyYx44XaFMuzvBkb55+Xhwh80t39ha4OOXyAFqtVtrADVQ9PT3Qav3qx4SIBuBXUy5Dhw6FxWLBnTt3+BF0TvL0R9ARkf/zq0DXaDQe/agzmV/+ydwbETmGr6WJiCTBQCcikgQDnYhIEgx0IiJJMNCJiCTBQCcikgQDnYhIEgx0IiJJMNCJiCTBQCcikgQDnYhIEgx0IiJJMNCJiCTBQCcikgQDnYhIEgx0IiJJMNCJiCTBQCcikgQDnYhIEgx0IiJJOPQh0VVVVSgoKICqqkhMTMSCBQts9p86dQqff/45IiMjAQBz5sxBYmKi+6slIqIB2Q10VVWRn5+PdevWQa/XY82aNTAajXjkkUdsxs2cOROpqakeK5SIiB7M7pRLbW0toqKiMHr0aCiKgpkzZ+L8+fPeqI2IiJxg9wy9ra0Ner3euq3X61FTU9Nn3Llz5/Drr7/i4YcfxquvvgqDwdBnTHFxMYqLiwEAWVlZ/Y7xJEVRvH5Mb5G5N0Du/thb4PK3/uwGuhCiz9c0Go3NdlxcHGbNmoWgoCAUFRUhNzcXmZmZfb4vKSkJSUlJ1u2WlhZXanaZwWDw+jG9RebeALn7Y2+Byxf9RUdHD7jP7pSLXq9Ha2urdbu1tRURERE2Y8LCwhAUFATgj9Cuq6tztVYiInKR3UCPiYmByWRCc3Mzenp6UF5eDqPRaDOmvb3d+veKioo+F0yJiMjz7E656HQ6pKSkYMuWLVBVFQkJCRgzZgwOHz6MmJgYGI1GHD9+HBUVFdDpdAgNDUVaWpo3aiciontoRH+T5F7S1NTk1ePJPJ8nc2+A3P2xt8AVcHPoREQUGBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAkGOhGRJBjoRESSYKATEUmCgU5EJAnFkUFVVVUoKCiAqqpITEzEggULbPZ3d3cjJycHdXV1CAsLQ3p6OkaNGuWRgomIqH92z9BVVUV+fj4yMjKwa9cunDlzBo2NjTZjSktLMWzYMOzduxfz5s3DF1984bGCiYiof3YDvba2FlFRURg9ejQURcHMmTNx/vx5mzEVFRWIj48HAMyYMQPV1dUQQnikYCIi6p/dKZe2tjbo9Xrrtl6vR01NzYBjdDodQkJC0NHRgeHDh9uMKy4uRnFxMQAgKysLBoNh0A04Q1EUrx/TW2TuDZC7P/YWuPytP7uB3t+ZtkajcXoMACQlJSEpKcm63dLS4lCR7mIwGLx+TG+RuTdA7v7YW+DyRX/R0dED7rM75aLX69Ha2mrdbm1tRURExIBjent70dnZidDQUFfrJSIiF9gN9JiYGJhMJjQ3N6Onpwfl5eUwGo02Y+Li4nDq1CkAwNmzZzFx4sR+z9CJiMhz7E656HQ6pKSkYMuWLVBVFQkJCRgzZgwOHz6MmJgYGI1GPPvss8jJycE777yD0NBQpKene6N2IiK6h0bwdhQiIin8pd4p+uGHH/q6BI+RuTdA7v7YW+Dyt/7+UoFORCQzBjoRkSR0Gzdu3OjrIrzpscce83UJHiNzb4Dc/bG3wOVP/fGiKBGRJDjlQkQkCQY6EZEkHFoPPdDIvH67vd6OHj2KkpIS6HQ6DB8+HMuWLcPIkSN9VK3z7PV319mzZ5GdnY1t27YhJibGy1W6xpHeysvLUVhYCI1Gg3HjxmHFihU+qNR59npraWlBbm4uzGYzVFXFkiVLMGXKFB9V65x9+/ahsrIS4eHh2LlzZ5/9QggUFBTg4sWLCA4ORlpamu/m1YVkent7xfLly8WNGzdEd3e3WLVqlWhoaLAZc+LECbF//34hhBA//PCDyM7O9kWpTnOkt59//llYLBYhhBAnT54MmN6EcKw/IYTo7OwUGzZsEBkZGaK2ttYHlTrPkd6amprE+++/Lzo6OoQQQty8edMXpTrNkd7y8vLEyZMnhRBCNDQ0iLS0NF+U6pJffvlFXLt2Tbz33nv97r9w4YLYsmWLUFVVXLlyRaxZs8bLFf5JuikXmddvd6S3J598EsHBwQCA2NhYtLW1+aJUlzjSHwAcPnwYycnJCAoK8kGVrnGkt5KSEjz//PPWhe3Cw8N9UarTHOlNo9Ggs7MTANDZ2dlngT9/9sQTTzxwscGKigrMnj0bGo0G48ePh9lsRnt7uxcr/JN0gd7f+u33h9pA67f7O0d6u1dpaSkmT57sjdLcwpH+6uvr0dLSgri4OG+XNyiO9NbU1ASTyYT169dj7dq1qKqq8naZLnGkt0WLFuH06dNYunQptm3bhpSUFG+X6TFtbW02a6Lb+730JOkCvb8zbVfXb/c3ztRdVlaGuro6JCcne7ost7HXn6qqOHjwIF555RVvluUWjjx3qqrCZDIhMzMTK1asQF5eHsxms7dKdJkjvZ05cwbx8fHIy8vDmjVrsHfvXqiq6q0SPcqf8kS6QJd5/XZHegOAS5cu4dtvv8Xq1asDalrCXn8WiwUNDQ3YtGkT3n77bdTU1GD79u24du2aL8p1iiPPXWRkJKZOnQpFUTBq1ChER0fDZDJ5u1SnOdJbaWkpnnnmGQDA+PHj0d3dHRCvih2h1+ttPuRioN9Lb5Au0GVev92R3urr63HgwAGsXr06YOZg77LXX0hICPLz85Gbm4vc3FzExsZi9erVAXGXiyPP3bRp01BdXQ0AuH37NkwmE0aPHu2Lcp3iSG8Gg8HaW2NjI7q7u/t8RGWgMhqNKCsrgxACV69eRUhIiM8CXcp3ilZWVuLgwYPW9dsXLlxos357V1cXcnJyUF9fb12/PRB+cQD7vW3evBnXr1/HiBEjAPzxi/TBBx/4uGrH2evvXhs3bsTLL78cEIEO2O9NCIFDhw6hqqoKWq0WCxcuxKxZs3xdtkPs9dbY2Ij9+/fDYrEAAF566SVMmjTJx1U7Zvfu3bh8+TI6OjoQHh6OxYsXo6enBwDw3HPPQQiB/Px8/PTTTxgyZAjS0tJ89jMpZaATEf0VSTflQkT0V8VAJyKSBAOdiEgSDHQiIkkw0ImIJMFAJyKSBAOdiEgS/w8AxfzX8+vK+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "model_name = \"model-1570490221\" # grab whichever model name you want here. We could also just reference the MODEL_NAME if you're in a notebook still.\n",
    "\n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, sample_type, acc, loss = c.split(\",\")\n",
    "\n",
    "            times.append(timestamp)\n",
    "            accuracies.append(acc)\n",
    "            losses.append(loss)\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"in_samp_acc\")\n",
    "    ax1.legend(loc=2)\n",
    "    ax2.plot(times,losses, label=\"in_samp_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "create_acc_loss_graph(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51 tensor(0.2500, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def test(size=32):\n",
    "    X, y = test_X[:size], test_y[:size]\n",
    "    val_acc, val_loss = fwd_pass(X.view(-1, 1, 50, 50).to(device), y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "val_acc, val_loss = test(size=100)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:16<00:00, 13.50it/s]\n",
      "100%|██████████| 225/225 [00:16<00:00, 13.56it/s]\n",
      "100%|██████████| 225/225 [00:17<00:00, 12.72it/s]\n",
      "100%|██████████| 225/225 [00:17<00:00, 12.84it/s]\n",
      "100%|██████████| 225/225 [00:17<00:00, 12.78it/s]\n",
      "100%|██████████| 225/225 [00:17<00:00, 12.89it/s]\n",
      "100%|██████████| 225/225 [00:17<00:00, 12.94it/s]\n",
      "100%|██████████| 225/225 [00:16<00:00, 13.31it/s]\n",
      "100%|██████████| 225/225 [00:17<00:00, 13.22it/s]\n",
      "100%|██████████| 225/225 [00:16<00:00, 13.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 10\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                #f.write(f\"{MODEL_NAME},{round(time.time(),3)},train,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i % 10 == 0:\n",
    "                    val_acc, val_loss = test(size=100)\n",
    "                    f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\")\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "%matplotlib qt\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "model_name = MODEL_NAME #\"model-1570499409\" # grab whichever model name you want here. We could also just reference the MODEL_NAME if you're in a notebook still.\n",
    "\n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, acc, loss, val_acc, val_loss = c.split(\",\")\n",
    "\n",
    "            times.append(float(timestamp))\n",
    "            accuracies.append(float(acc))\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            val_accs.append(float(val_acc))\n",
    "            val_losses.append(float(val_loss))\n",
    "\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"acc\")\n",
    "    ax1.plot(times, val_accs, label=\"val_acc\")\n",
    "    ax1.legend(loc=2)\n",
    "    ax2.plot(times,losses, label=\"loss\")\n",
    "    ax2.plot(times,val_losses, label=\"val_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "create_acc_loss_graph(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('gpupytorc': conda)",
   "language": "python",
   "name": "python37764bitgpupytorccondaf59b79b220374cd9932702741fe76c47"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
