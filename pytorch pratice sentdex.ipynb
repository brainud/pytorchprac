{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5,3])\n",
    "y = torch.tensor([2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  3])\n"
     ]
    }
   ],
   "source": [
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros([2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand([2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8683, 0.2619, 0.8955, 0.5000, 0.2895],\n",
      "        [0.8198, 0.5414, 0.1556, 0.5667, 0.0264]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8683, 0.2619, 0.8955, 0.5000, 0.2895, 0.8198, 0.5414, 0.1556, 0.5667,\n",
       "         0.0264]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view([1,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8683, 0.2619, 0.8955, 0.5000, 0.2895],\n",
      "        [0.8198, 0.5414, 0.1556, 0.5667, 0.0264]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.view([1,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8683, 0.2619, 0.8955, 0.5000, 0.2895, 0.8198, 0.5414, 0.1556, 0.5667,\n",
      "         0.0264]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple mathmematics with array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most time will be use in prepping data\n",
    "# Batching\n",
    "train = datasets.MNIST(\"data/\", train = True, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# STOP finding higher curve learning content, just keep practice\n",
    "# Tedious work to iterate over data\n",
    "# for validation, we need out of sample data. Cause machine will keep overfitting\n",
    "# on in-sample data\n",
    "\n",
    "# Open parenthesis is for where you want your data to go, for going locally, nothing to specify\n",
    "test = datasets.MNIST(\"data/\", train = False, download=True,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding 10 at a time, base 8 number for higher batch size, hoping this data will generalize\n",
    "# Batching helps regulariztion\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size =10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size =10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0, 6, 3, 5, 5, 2, 9, 6, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# the last temporary variable in for loop can still be accessible\n",
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)\n",
    "#shaping stuff is important, there is unnecessary 1 at the start for right dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models will just try top decrease loss\n",
    "# always check for balancing dataset\n",
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    Xs, ys =data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total +=1\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:9.871666666666666\n",
      "1:11.236666666666666\n",
      "2:9.93\n",
      "3:10.218333333333334\n",
      "4:9.736666666666666\n",
      "5:9.035\n",
      "6:9.863333333333333\n",
      "7:10.441666666666666\n",
      "8:9.751666666666667\n",
      "9:9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}:{counter_dict[i]/ total*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# If you want the init method of your inherited class to run use super init\n",
    "# calling paste as pasta\n",
    "input = 28 * 28\n",
    "output = 64     # hidden layer ouput\n",
    "final_output = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # forgetting super.__init__ will lead to attribute error\n",
    "        #fc1 is fully connected\n",
    "        self.fc1 = nn.Linear(input, output)\n",
    "        self.fc2 = nn.Linear(output, output)\n",
    "        self.fc3 = nn.Linear(output, output)\n",
    "        self.fc4 = nn.Linear(output, final_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  #helper function of pytorch\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim =1) # dim is almost same as axes dim = 0 might means distributing across batch\n",
    "    \n",
    "        # you can do fancy things here apart from other libraries, throw logic in here, really advance models, here with pytorch it's really simple and autograd is a boon\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class distribution we require probability, here we use log softmax\n",
    "X = torch.rand([28,28])\n",
    "X = X.view(-1 or 0, 28*28) #flatten\n",
    "# -1 : meaning it's of any size adding array\n",
    "# you have to format things exactly how these library exactly want it to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2814, -2.3366, -2.4560, -2.2617, -2.2289, -2.4409, -2.2259, -2.2929,\n",
       "         -2.2014, -2.3337]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = torch.rand([3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6493, 0.4570, 0.6371],\n",
       "        [0.5427, 0.6918, 0.6004],\n",
       "        [0.1213, 0.2206, 0.3495]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6493, 0.4570, 0.6371, 0.5427, 0.6918, 0.6004, 0.1213, 0.2206, 0.3495])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.view(3*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6493, 0.4570, 0.6371, 0.5427, 0.6918, 0.6004, 0.1213, 0.2206, 0.3495]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.view(1,3*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6493, 0.4570, 0.6371, 0.5427, 0.6918, 0.6004, 0.1213, 0.2206, 0.3495]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j.view(-1,3*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6493, 0.4570, 0.6371],\n",
       "        [0.5427, 0.6918, 0.6004],\n",
       "        [0.1213, 0.2206, 0.3495]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2041, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0056, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001) #1e-3 lr says the step taken by optimizer\n",
    "# maybe overfitting is the reason why we don't need 100\n",
    "\n",
    "# net.parameters means it will take anything that is adjustable in our model\n",
    "# a full pass through data is called an epoch\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        # data is a batch of featuresets and labels\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28 * 28)) # input of flattened data image pixels\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward() #backprop\n",
    "        optimizer.step()\n",
    "    print(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.977\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): # we don't want any sort of gradient calculation in background rn\n",
    "    # torch will continously keep taking grad\n",
    "    # historically there was net.train()...... and net.eval \n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        for idx, i in enumerate(output): # enumerate gives out counter and value from a list\n",
    "            if torch.argmax(i) == y[idx]: #torch.argmax returns the index of maximum value\n",
    "                correct +=1\n",
    "            total +=1\n",
    "            \n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's really easy to sneak in the data leakage\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[7].view(28,28))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[7].view(-1,28*28))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution NN working almost better at series dat than \n",
    "# Ignite package for pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
